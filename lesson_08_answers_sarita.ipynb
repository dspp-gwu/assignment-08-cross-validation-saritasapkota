{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8: Cross-validation\n",
    "\n",
    "So far, we've learned about splitting our data into training and testing sets to validate our models. This helps ensure that the model we create on one sample performs well on another sample we want to predict. \n",
    "\n",
    "However, we don't have to use just TWO samples to train and test our models. Instead, we can split our data up into MULTIPLE samples to train and test on multiple segments of the data. This is called CROSS-VALIDATION. This allows us to ensure that our model predicts outcomes over a wider range of circumstances. \n",
    "\n",
    "Let's begin by importing our packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "# packages in environment at /opt/conda:\n",
      "#\n",
      "geopandas                 0.3.0                    py36_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "! conda install geopandas -yq\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll be looking at 311 service requests for rodent inspection and abatement aggregated at the Census block level. The data set is already prepared for you and available in the same folder as this assignment. Census blocks are a good geographic level to analyze rodent infestations because they are drawn along natural and human-made boundaries, like rivers and roads, that rats tend not to cross. \n",
    "\n",
    "We will look at the 'activity' variable, which indicates whether inspectors found rat burrows during an inspection (1) or not (0). Here we are looking only at inpsections in 2016. About 43 percent on inspections in 2016 led to inspectors finding and treating rat burrows, as you can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('rat_data_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['activity', 'alley_condition', 'bbl_hotel', 'bbl_multifamily_rental',\n",
       "       'bbl_restaurant', 'bbl_single_family_rental', 'bbl_storage',\n",
       "       'bbl_two_family_rental', 'communitygarden_area', 'communitygarden_id',\n",
       "       'dcrapermit_addition', 'dcrapermit_demolition', 'dcrapermit_excavation',\n",
       "       'dcrapermit_new_building', 'dcrapermit_raze', 'impervious_area',\n",
       "       'month', 'num_mixed_use', 'num_non_residential', 'num_residential',\n",
       "       'park', 'pct_mixed_use', 'pct_non_residential', 'pct_residential',\n",
       "       'pop_density', 'sidewalk_grates', 'ssl_cndtn_Average_comm',\n",
       "       'ssl_cndtn_Average_res', 'ssl_cndtn_Excellent_comm',\n",
       "       'ssl_cndtn_Excellent_res', 'ssl_cndtn_Fair_comm', 'ssl_cndtn_Fair_res',\n",
       "       'ssl_cndtn_Good_comm', 'ssl_cndtn_Good_res', 'ssl_cndtn_Poor_comm',\n",
       "       'ssl_cndtn_Poor_res', 'ssl_cndtn_VeryGood_comm',\n",
       "       'ssl_cndtn_VeryGood_res', 'tot_pop', 'well_activity', 'WARD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>activity</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.431696</td>\n",
       "      <td>0.495408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alley_condition</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>11.111282</td>\n",
       "      <td>8.900166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbl_hotel</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.082118</td>\n",
       "      <td>0.376073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbl_multifamily_rental</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>1.388718</td>\n",
       "      <td>2.376244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbl_restaurant</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.569455</td>\n",
       "      <td>1.518526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbl_single_family_rental</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>4.709133</td>\n",
       "      <td>8.375165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbl_storage</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.051768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbl_two_family_rental</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.743668</td>\n",
       "      <td>1.378860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communitygarden_area</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>18.727920</td>\n",
       "      <td>326.332382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11004.319881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communitygarden_id</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.242134</td>\n",
       "      <td>3.234069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcrapermit_addition</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.054490</td>\n",
       "      <td>0.265959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcrapermit_demolition</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.280507</td>\n",
       "      <td>0.738300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcrapermit_excavation</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcrapermit_new_building</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.053722</td>\n",
       "      <td>0.286938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcrapermit_raze</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.044896</td>\n",
       "      <td>0.308390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impervious_area</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>18450.549489</td>\n",
       "      <td>18774.921307</td>\n",
       "      <td>2150.037473</td>\n",
       "      <td>11356.602831</td>\n",
       "      <td>14532.464194</td>\n",
       "      <td>19920.166029</td>\n",
       "      <td>473222.487756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>7.194935</td>\n",
       "      <td>3.001022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_mixed_use</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.153876</td>\n",
       "      <td>0.474006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_non_residential</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>3.804682</td>\n",
       "      <td>5.956966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>58.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_residential</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>39.287797</td>\n",
       "      <td>26.661136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>334.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>park</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.046815</td>\n",
       "      <td>0.225350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_mixed_use</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.003899</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_non_residential</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.139389</td>\n",
       "      <td>0.242231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pct_residential</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.854794</td>\n",
       "      <td>0.247308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838200</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop_density</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>24969.466549</td>\n",
       "      <td>19217.566990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13524.013052</td>\n",
       "      <td>21965.055541</td>\n",
       "      <td>30907.352412</td>\n",
       "      <td>182709.507271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sidewalk_grates</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>2.109363</td>\n",
       "      <td>5.186348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Average_comm</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.402058</td>\n",
       "      <td>0.401132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Average_res</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.574651</td>\n",
       "      <td>0.265928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.635642</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Excellent_comm</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.022413</td>\n",
       "      <td>0.095254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Excellent_res</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.029081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Fair_comm</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.024810</td>\n",
       "      <td>0.101711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Fair_res</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.047230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Good_comm</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.162147</td>\n",
       "      <td>0.261734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Good_res</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.285093</td>\n",
       "      <td>0.206829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Poor_comm</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_Poor_res</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_VeryGood_comm</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.104810</td>\n",
       "      <td>0.232111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssl_cndtn_VeryGood_res</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.071472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tot_pop</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>188.892172</td>\n",
       "      <td>211.199274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>3888.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well_activity</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>0.572141</td>\n",
       "      <td>1.569373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WARD</th>\n",
       "      <td>2606.0</td>\n",
       "      <td>3.994244</td>\n",
       "      <td>2.112836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count          mean           std          min  \\\n",
       "activity                  2606.0      0.431696      0.495408     0.000000   \n",
       "alley_condition           2606.0     11.111282      8.900166     0.000000   \n",
       "bbl_hotel                 2606.0      0.082118      0.376073     0.000000   \n",
       "bbl_multifamily_rental    2606.0      1.388718      2.376244     0.000000   \n",
       "bbl_restaurant            2606.0      0.569455      1.518526     0.000000   \n",
       "bbl_single_family_rental  2606.0      4.709133      8.375165     0.000000   \n",
       "bbl_storage               2606.0      0.002686      0.051768     0.000000   \n",
       "bbl_two_family_rental     2606.0      0.743668      1.378860     0.000000   \n",
       "communitygarden_area      2606.0     18.727920    326.332382     0.000000   \n",
       "communitygarden_id        2606.0      0.242134      3.234069     0.000000   \n",
       "dcrapermit_addition       2606.0      0.054490      0.265959     0.000000   \n",
       "dcrapermit_demolition     2606.0      0.280507      0.738300     0.000000   \n",
       "dcrapermit_excavation     2606.0      0.010361      0.105000     0.000000   \n",
       "dcrapermit_new_building   2606.0      0.053722      0.286938     0.000000   \n",
       "dcrapermit_raze           2606.0      0.044896      0.308390     0.000000   \n",
       "impervious_area           2606.0  18450.549489  18774.921307  2150.037473   \n",
       "month                     2606.0      7.194935      3.001022     1.000000   \n",
       "num_mixed_use             2606.0      0.153876      0.474006     0.000000   \n",
       "num_non_residential       2606.0      3.804682      5.956966     0.000000   \n",
       "num_residential           2606.0     39.287797     26.661136     0.000000   \n",
       "park                      2606.0      0.046815      0.225350     0.000000   \n",
       "pct_mixed_use             2606.0      0.003899      0.013706     0.000000   \n",
       "pct_non_residential       2606.0      0.139389      0.242231     0.000000   \n",
       "pct_residential           2606.0      0.854794      0.247308     0.000000   \n",
       "pop_density               2606.0  24969.466549  19217.566990     0.000000   \n",
       "sidewalk_grates           2606.0      2.109363      5.186348     0.000000   \n",
       "ssl_cndtn_Average_comm    2606.0      0.402058      0.401132     0.000000   \n",
       "ssl_cndtn_Average_res     2606.0      0.574651      0.265928     0.000000   \n",
       "ssl_cndtn_Excellent_comm  2606.0      0.022413      0.095254     0.000000   \n",
       "ssl_cndtn_Excellent_res   2606.0      0.001871      0.029081     0.000000   \n",
       "ssl_cndtn_Fair_comm       2606.0      0.024810      0.101711     0.000000   \n",
       "ssl_cndtn_Fair_res        2606.0      0.018239      0.047230     0.000000   \n",
       "ssl_cndtn_Good_comm       2606.0      0.162147      0.261734     0.000000   \n",
       "ssl_cndtn_Good_res        2606.0      0.285093      0.206829     0.000000   \n",
       "ssl_cndtn_Poor_comm       2606.0      0.002487      0.032450     0.000000   \n",
       "ssl_cndtn_Poor_res        2606.0      0.002067      0.010527     0.000000   \n",
       "ssl_cndtn_VeryGood_comm   2606.0      0.104810      0.232111     0.000000   \n",
       "ssl_cndtn_VeryGood_res    2606.0      0.036727      0.071472     0.000000   \n",
       "tot_pop                   2606.0    188.892172    211.199274     0.000000   \n",
       "well_activity             2606.0      0.572141      1.569373     0.000000   \n",
       "WARD                      2606.0      3.994244      2.112836     1.000000   \n",
       "\n",
       "                                   25%           50%           75%  \\\n",
       "activity                      0.000000      0.000000      1.000000   \n",
       "alley_condition               4.000000     10.000000     16.000000   \n",
       "bbl_hotel                     0.000000      0.000000      0.000000   \n",
       "bbl_multifamily_rental        0.000000      0.000000      2.000000   \n",
       "bbl_restaurant                0.000000      0.000000      0.000000   \n",
       "bbl_single_family_rental      1.000000      2.000000      5.000000   \n",
       "bbl_storage                   0.000000      0.000000      0.000000   \n",
       "bbl_two_family_rental         0.000000      0.000000      1.000000   \n",
       "communitygarden_area          0.000000      0.000000      0.000000   \n",
       "communitygarden_id            0.000000      0.000000      0.000000   \n",
       "dcrapermit_addition           0.000000      0.000000      0.000000   \n",
       "dcrapermit_demolition         0.000000      0.000000      0.000000   \n",
       "dcrapermit_excavation         0.000000      0.000000      0.000000   \n",
       "dcrapermit_new_building       0.000000      0.000000      0.000000   \n",
       "dcrapermit_raze               0.000000      0.000000      0.000000   \n",
       "impervious_area           11356.602831  14532.464194  19920.166029   \n",
       "month                         5.000000      7.000000     10.000000   \n",
       "num_mixed_use                 0.000000      0.000000      0.000000   \n",
       "num_non_residential           0.000000      1.000000      5.000000   \n",
       "num_residential              21.000000     36.000000     53.000000   \n",
       "park                          0.000000      0.000000      0.000000   \n",
       "pct_mixed_use                 0.000000      0.000000      0.000000   \n",
       "pct_non_residential           0.000000      0.038462      0.150000   \n",
       "pct_residential               0.838200      0.959184      1.000000   \n",
       "pop_density               13524.013052  21965.055541  30907.352412   \n",
       "sidewalk_grates               0.000000      0.000000      2.000000   \n",
       "ssl_cndtn_Average_comm        0.000000      0.333333      0.800000   \n",
       "ssl_cndtn_Average_res         0.444444      0.635642      0.767857   \n",
       "ssl_cndtn_Excellent_comm      0.000000      0.000000      0.000000   \n",
       "ssl_cndtn_Excellent_res       0.000000      0.000000      0.000000   \n",
       "ssl_cndtn_Fair_comm           0.000000      0.000000      0.000000   \n",
       "ssl_cndtn_Fair_res            0.000000      0.000000      0.017857   \n",
       "ssl_cndtn_Good_comm           0.000000      0.000000      0.250000   \n",
       "ssl_cndtn_Good_res            0.133333      0.266667      0.403509   \n",
       "ssl_cndtn_Poor_comm           0.000000      0.000000      0.000000   \n",
       "ssl_cndtn_Poor_res            0.000000      0.000000      0.000000   \n",
       "ssl_cndtn_VeryGood_comm       0.000000      0.000000      0.000000   \n",
       "ssl_cndtn_VeryGood_res        0.000000      0.000000      0.052632   \n",
       "tot_pop                      82.000000    137.000000    231.000000   \n",
       "well_activity                 0.000000      0.000000      0.000000   \n",
       "WARD                          2.000000      4.000000      6.000000   \n",
       "\n",
       "                                    max  \n",
       "activity                       1.000000  \n",
       "alley_condition               79.000000  \n",
       "bbl_hotel                      8.000000  \n",
       "bbl_multifamily_rental        22.000000  \n",
       "bbl_restaurant                17.000000  \n",
       "bbl_single_family_rental     147.000000  \n",
       "bbl_storage                    1.000000  \n",
       "bbl_two_family_rental         15.000000  \n",
       "communitygarden_area       11004.319881  \n",
       "communitygarden_id            80.000000  \n",
       "dcrapermit_addition            4.000000  \n",
       "dcrapermit_demolition         12.000000  \n",
       "dcrapermit_excavation          2.000000  \n",
       "dcrapermit_new_building        4.000000  \n",
       "dcrapermit_raze                5.000000  \n",
       "impervious_area           473222.487756  \n",
       "month                         12.000000  \n",
       "num_mixed_use                  4.000000  \n",
       "num_non_residential           58.000000  \n",
       "num_residential              334.000000  \n",
       "park                           4.000000  \n",
       "pct_mixed_use                  0.250000  \n",
       "pct_non_residential            1.000000  \n",
       "pct_residential                1.000000  \n",
       "pop_density               182709.507271  \n",
       "sidewalk_grates               73.000000  \n",
       "ssl_cndtn_Average_comm         1.000000  \n",
       "ssl_cndtn_Average_res          1.000000  \n",
       "ssl_cndtn_Excellent_comm       1.000000  \n",
       "ssl_cndtn_Excellent_res        0.766990  \n",
       "ssl_cndtn_Fair_comm            1.000000  \n",
       "ssl_cndtn_Fair_res             0.666667  \n",
       "ssl_cndtn_Good_comm            1.000000  \n",
       "ssl_cndtn_Good_res             1.000000  \n",
       "ssl_cndtn_Poor_comm            1.000000  \n",
       "ssl_cndtn_Poor_res             0.200000  \n",
       "ssl_cndtn_VeryGood_comm        1.000000  \n",
       "ssl_cndtn_VeryGood_res         1.000000  \n",
       "tot_pop                     3888.000000  \n",
       "well_activity                 19.000000  \n",
       "WARD                           8.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from last week that, when we do predictive analysis, we usually are not interested in the relationship between two different variables as we are when we do traditional hypothesis testing. Instead, we're interested in training a model that generates predictions that best fit our target population. Therefore, when we are doing any kind of validation, including cross-validation, it is important for us to choose the metric by which we will evaluate the performance of our models. \n",
    "\n",
    "For this model, we will predict the locations of requests for rodent inspection and abatement in the District of Columbia. When we select a validation metric, it's important for us to think about what we want to optimize. For example, do we want to make sure that our top predictions accurately identify places with rodent infestations, so we don't send our inspectors on a wild goose chase? Then we may to look at the models precision, or what proportion of its positive predictions turn out to be positive. Or do we want to make sure we don't miss any infestations? If so, we may want to look at recall, or the proportion of positive cases that are correctly categorized by the model. If we care a lot about how the model ranks our observations, then we may want to look at the area under the ROC curve, or ROC-AUC, while if we care more about how well the model fits the data, or its \"calibration,\" we may want to look at Brier score or logarithmic loss (log-loss).\n",
    "\n",
    "In the case of rodent inspections, we most likely want to make sure that we send our inspectors to places where they are most likely to find rats and to avoid sending them on wild goose [rat] chases. Therefore, we will optimize for precision, which we will call from the metrics library in scikit-learn. \n",
    "\n",
    "The metrics library in scikit-learn provides a number of different options. You should take some time to look at the different metrics that are available to you and consider which ones are most appropriate for your own research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next important decision we need to make when cross-validating our models is how we will define our \"folds.\" Folds are the independent subsamples on which we train and test the data. Keep in mind that it is important that our folds are INDEPENDENT, which means we must guarantee that there's no overlap between our training and test set (i.e., no observation is in both the training and test set). Independence can also have other implications for how we slice the data, which we will discuss as we progress through this lesson.\n",
    "\n",
    "One of the most common approaches to cross-validation is to make random splits in the data. This is often referred to as k-fold cross-validation, in which the only thing we define is the number of folds (k) that want to split our sample into. Here, I'll use the KFold function from scikit-learn's model_selection library. Let's begin by importing the library and then taking a look at how it splits our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KFold divides our data into a pre-specified number of (approximately) equally-sized folds so that each observation is in the test set once. When we specify that shuffle=True, KFold first shuffles our data into a random order to ensure that the observations are randomly selected. By selecting a random_state, we can ensure that KFold selects observations the same way each time. \n",
    "\n",
    "While there are other functions in the model_selection library that will do much of this work for us, KFold will allow us to look at what's going on in the background of our cross-validation process. Let's begin by just looking at how KFold splits our data. Here we split our data into 10 folds each with 10 percent of the data (.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   9   22   27   33   53   70   92  104  109  117  121  135  137  156  182\n",
      "  192  195  196  215  217  224  227  252  259  271  276  289  314  317  326\n",
      "  333  351  398  399  418  422  427  436  438  443  452  465  478  480  482\n",
      "  489  518  547  562  563  567  569  578  581  597  609  616  618  619  674\n",
      "  682  686  700  704  710  711  720  722  728  743  745  746  748  764  778\n",
      "  795  817  831  855  868  878  880  899  913  916  921  927  933  961  962\n",
      "  982  983  988  998 1000 1012 1013 1018 1023 1032 1036 1051 1052 1059 1078\n",
      " 1079 1096 1100 1101 1106 1108 1109 1147 1187 1192 1213 1263 1264 1285 1287\n",
      " 1300 1323 1326 1327 1371 1396 1418 1421 1432 1452 1484 1507 1515 1520 1544\n",
      " 1568 1570 1577 1580 1585 1588 1590 1592 1597 1622 1627 1656 1657 1668 1680\n",
      " 1681 1686 1689 1708 1710 1719 1729 1732 1735 1757 1761 1762 1763 1765 1780\n",
      " 1783 1790 1798 1803 1814 1816 1818 1820 1821 1827 1832 1836 1853 1873 1874\n",
      " 1895 1898 1901 1902 1921 1927 1929 1939 1943 1961 1968 1969 1972 1974 1979\n",
      " 1982 1989 1997 1998 2019 2027 2030 2033 2055 2074 2085 2087 2090 2115 2123\n",
      " 2134 2138 2151 2156 2161 2183 2186 2191 2225 2230 2254 2258 2274 2281 2289\n",
      " 2302 2312 2316 2317 2343 2345 2346 2358 2359 2378 2386 2387 2398 2400 2410\n",
      " 2416 2417 2419 2429 2450 2468 2498 2499 2502 2510 2520 2543 2544 2550 2555\n",
      " 2564 2567 2579 2585 2596 2597]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   4   10   14   23   37   39   41   57   69   76   98  113  124  132  145\n",
      "  148  162  179  191  204  232  234  245  248  251  296  302  303  311  320\n",
      "  330  353  357  361  379  385  390  402  405  414  425  440  446  454  457\n",
      "  477  486  487  501  526  527  536  543  558  565  582  610  615  621  634\n",
      "  638  652  653  666  667  672  676  687  688  692  702  703  708  712  713\n",
      "  715  716  758  776  789  812  828  838  840  847  852  876  891  897  898\n",
      "  905  909  914  924  926  935  966  989  997 1002 1003 1025 1041 1068 1070\n",
      " 1091 1093 1110 1118 1122 1138 1146 1150 1161 1173 1185 1193 1197 1199 1210\n",
      " 1211 1222 1228 1231 1232 1239 1242 1244 1270 1292 1294 1303 1332 1334 1362\n",
      " 1366 1377 1380 1405 1412 1414 1424 1426 1449 1465 1467 1486 1487 1493 1496\n",
      " 1504 1506 1512 1525 1535 1539 1543 1548 1549 1553 1555 1573 1594 1599 1601\n",
      " 1625 1637 1642 1646 1663 1664 1665 1675 1678 1702 1703 1712 1714 1727 1728\n",
      " 1768 1770 1774 1781 1809 1813 1824 1825 1826 1839 1845 1851 1852 1859 1885\n",
      " 1928 1937 1946 1949 1952 1955 1988 2001 2007 2013 2037 2038 2045 2054 2067\n",
      " 2069 2073 2088 2095 2116 2131 2143 2149 2175 2201 2207 2213 2229 2233 2245\n",
      " 2246 2262 2266 2272 2276 2288 2293 2297 2321 2333 2341 2354 2356 2376 2381\n",
      " 2395 2399 2405 2409 2442 2482 2491 2497 2508 2512 2515 2526 2531 2556 2559\n",
      " 2561 2562 2566 2588 2592 2594]\n",
      "TRAIN: [   0    2    3 ..., 2603 2604 2605] TEST: [   1    6   11   17   18   30   40   47   48   52   58  107  125  133  149\n",
      "  157  161  173  175  189  194  200  206  220  229  249  254  264  283  286\n",
      "  300  305  322  342  384  386  391  392  411  442  444  453  458  459  461\n",
      "  488  503  505  517  529  530  535  538  557  568  570  574  575  579  587\n",
      "  596  602  641  646  648  651  657  661  665  670  684  723  727  731  757\n",
      "  762  768  792  805  841  850  886  892  895  900  906  918  936  949  951\n",
      "  953  963  995  996 1005 1009 1015 1017 1027 1042 1055 1058 1063 1073 1081\n",
      " 1098 1103 1116 1126 1127 1139 1140 1157 1160 1174 1188 1190 1203 1205 1226\n",
      " 1236 1246 1256 1267 1271 1273 1283 1295 1302 1317 1322 1328 1357 1367 1370\n",
      " 1373 1386 1393 1411 1422 1428 1431 1448 1450 1459 1471 1492 1503 1513 1521\n",
      " 1523 1528 1533 1540 1547 1567 1569 1598 1602 1621 1631 1632 1643 1673 1677\n",
      " 1697 1704 1713 1715 1724 1725 1726 1736 1737 1753 1754 1773 1802 1819 1829\n",
      " 1840 1854 1855 1861 1864 1867 1869 1872 1897 1900 1931 1942 1945 1947 1964\n",
      " 1983 1985 1996 1999 2012 2015 2026 2039 2052 2072 2083 2097 2099 2109 2124\n",
      " 2144 2145 2165 2188 2190 2198 2208 2223 2235 2238 2240 2249 2256 2279 2284\n",
      " 2285 2290 2300 2308 2320 2326 2336 2352 2355 2357 2367 2372 2382 2396 2407\n",
      " 2414 2422 2430 2432 2462 2467 2471 2477 2480 2495 2500 2511 2518 2525 2548\n",
      " 2549 2560 2569 2576 2582 2601]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  15   31   34   43   61   64   77   80   85   87  106  118  139  141  144\n",
      "  169  187  202  223  233  240  250  253  260  262  267  270  279  287  294\n",
      "  295  298  299  309  310  359  360  369  376  381  393  415  474  475  483\n",
      "  485  491  502  506  512  516  519  521  522  532  546  553  564  572  600\n",
      "  620  629  633  636  643  654  655  663  701  721  733  735  766  775  781\n",
      "  791  793  794  799  806  810  815  818  820  825  829  832  836  842  861\n",
      "  882  883  890  896  910  917  923  937  938  944  946  958  964  971  977\n",
      "  979  980  981  986  991 1010 1038 1043 1045 1047 1049 1056 1074 1077 1083\n",
      " 1087 1097 1099 1114 1119 1136 1148 1151 1170 1180 1183 1212 1217 1240 1254\n",
      " 1255 1279 1301 1325 1330 1339 1341 1347 1355 1368 1374 1385 1388 1390 1397\n",
      " 1399 1417 1453 1458 1474 1485 1490 1491 1516 1559 1560 1571 1581 1587 1593\n",
      " 1603 1610 1611 1647 1648 1674 1694 1696 1709 1758 1759 1760 1779 1786 1787\n",
      " 1789 1801 1808 1817 1860 1865 1875 1876 1892 1903 1907 1916 1919 1923 1935\n",
      " 1948 1980 1994 2020 2032 2044 2050 2053 2057 2061 2063 2064 2075 2078 2092\n",
      " 2122 2129 2133 2140 2158 2164 2173 2178 2203 2204 2210 2219 2259 2263 2273\n",
      " 2296 2298 2305 2311 2319 2328 2351 2360 2391 2392 2394 2397 2401 2402 2406\n",
      " 2408 2413 2441 2444 2447 2457 2458 2469 2478 2479 2493 2507 2509 2519 2522\n",
      " 2536 2538 2571 2574 2575 2581]\n",
      "TRAIN: [   0    1    3 ..., 2602 2604 2605] TEST: [   2    5   19   36   42   45   54   55   65   66   72   73   82   89  102\n",
      "  108  122  140  142  152  154  159  170  171  177  178  184  185  190  198\n",
      "  203  210  214  219  268  272  278  308  312  315  318  319  335  340  341\n",
      "  347  364  378  383  408  412  416  434  467  468  473  479  481  498  511\n",
      "  534  539  551  561  583  590  598  632  644  658  689  717  718  724  726\n",
      "  740  744  750  759  760  772  773  785  796  801  811  813  823  839  856\n",
      "  863  866  893  930  955  965  974  978  985  987  992 1001 1008 1021 1026\n",
      " 1029 1050 1069 1076 1080 1082 1117 1129 1132 1135 1137 1145 1154 1165 1166\n",
      " 1175 1195 1216 1225 1235 1257 1259 1261 1266 1275 1277 1280 1284 1293 1338\n",
      " 1343 1349 1358 1359 1363 1376 1378 1379 1387 1423 1427 1454 1456 1457 1460\n",
      " 1462 1473 1478 1482 1489 1499 1500 1518 1526 1530 1537 1538 1550 1606 1612\n",
      " 1615 1633 1635 1661 1666 1679 1717 1742 1748 1749 1752 1764 1775 1785 1846\n",
      " 1848 1858 1878 1886 1890 1909 1914 1934 1950 1957 1960 1976 2000 2005 2010\n",
      " 2017 2018 2025 2028 2031 2034 2035 2056 2058 2093 2100 2102 2108 2119 2160\n",
      " 2166 2172 2181 2194 2202 2218 2228 2236 2242 2244 2248 2253 2264 2265 2267\n",
      " 2269 2270 2278 2283 2303 2310 2324 2325 2349 2364 2365 2393 2421 2434 2438\n",
      " 2452 2453 2456 2460 2463 2465 2475 2484 2485 2487 2492 2505 2506 2514 2528\n",
      " 2532 2552 2570 2598 2599 2603]\n",
      "TRAIN: [   0    1    2 ..., 2602 2603 2605] TEST: [   8   13   16   29   32   35   49   56   60   68   71   75   96  110  114\n",
      "  134  147  155  186  211  212  218  226  231  241  243  244  247  258  265\n",
      "  285  293  304  336  339  354  355  358  362  367  372  377  401  406  420\n",
      "  426  431  432  435  445  450  455  456  464  471  494  499  500  513  520\n",
      "  524  533  540  542  548  549  552  554  576  580  585  589  593  611  631\n",
      "  642  649  668  677  678  679  693  706  725  737  752  769  782  788  803\n",
      "  814  862  871  875  881  884  887  889  939  940  942  943  948  957  960\n",
      "  970  984  994  999 1014 1016 1031 1044 1060 1064 1065 1075 1105 1120 1123\n",
      " 1124 1128 1143 1164 1171 1184 1189 1200 1218 1227 1229 1245 1248 1252 1265\n",
      " 1299 1310 1312 1315 1319 1336 1340 1354 1361 1403 1407 1436 1444 1446 1451\n",
      " 1455 1463 1464 1476 1495 1498 1501 1505 1511 1519 1554 1572 1586 1591 1604\n",
      " 1607 1626 1644 1650 1652 1654 1655 1662 1669 1670 1683 1698 1716 1721 1738\n",
      " 1739 1741 1746 1766 1767 1771 1776 1794 1806 1837 1850 1870 1880 1888 1893\n",
      " 1894 1922 1936 1944 1951 1953 1981 1984 1992 2016 2029 2043 2060 2062 2065\n",
      " 2070 2082 2096 2111 2125 2126 2130 2132 2157 2167 2174 2192 2193 2196 2205\n",
      " 2216 2224 2231 2243 2250 2255 2268 2301 2307 2314 2323 2330 2337 2363 2368\n",
      " 2403 2404 2423 2436 2440 2461 2466 2481 2516 2537 2540 2541 2547 2557 2563\n",
      " 2572 2580 2586 2589 2600 2604]\n",
      "TRAIN: [   0    1    2 ..., 2602 2603 2604] TEST: [  38   44   51   59   78   81   83   88   97  103  111  119  123  129  131\n",
      "  165  183  188  205  213  238  239  261  263  269  313  316  328  338  349\n",
      "  356  363  371  374  382  395  397  410  439  448  463  466  472  476  484\n",
      "  490  493  507  510  514  528  541  571  599  601  608  613  625  635  645\n",
      "  656  660  681  685  695  697  729  751  761  771  779  783  784  808  819\n",
      "  826  833  844  846  849  853  858  867  874  879  904  907  911  919  934\n",
      "  947  969  993 1019 1034 1035 1039 1057 1067 1084 1088 1089 1094 1102 1115\n",
      " 1121 1125 1156 1158 1191 1196 1220 1223 1224 1233 1238 1260 1262 1268 1269\n",
      " 1274 1276 1281 1290 1291 1296 1307 1311 1318 1320 1321 1324 1335 1344 1351\n",
      " 1360 1364 1375 1382 1383 1391 1401 1408 1410 1415 1419 1420 1430 1439 1440\n",
      " 1442 1475 1477 1494 1502 1510 1524 1527 1529 1534 1557 1564 1609 1613 1614\n",
      " 1618 1624 1630 1636 1639 1651 1658 1687 1691 1693 1695 1700 1706 1711 1733\n",
      " 1743 1756 1772 1784 1804 1811 1830 1831 1833 1841 1842 1857 1881 1882 1887\n",
      " 1906 1912 1918 1959 1977 1991 2004 2014 2086 2094 2101 2103 2104 2106 2110\n",
      " 2114 2118 2127 2147 2179 2184 2199 2214 2220 2226 2227 2232 2239 2241 2275\n",
      " 2286 2287 2294 2318 2327 2329 2332 2342 2348 2373 2375 2411 2420 2428 2433\n",
      " 2437 2439 2451 2459 2464 2472 2474 2489 2503 2521 2529 2530 2534 2535 2545\n",
      " 2546 2551 2553 2590 2605]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  12   20   28   46   50   62   74   79   90   95  101  105  115  116  120\n",
      "  127  128  138  143  150  158  167  172  181  193  208  222  225  228  230\n",
      "  235  236  242  255  266  284  288  290  301  306  331  332  337  344  345\n",
      "  346  352  366  370  380  389  394  396  403  409  413  417  421  441  462\n",
      "  492  495  496  515  523  531  545  559  566  588  592  612  614  617  622\n",
      "  626  628  630  662  669  675  683  690  707  719  734  742  747  753  765\n",
      "  777  787  790  798  822  824  857  864  870  877  901  912  915  920  922\n",
      "  959  968 1024 1030 1037 1054 1061 1062 1066 1072 1086 1092 1095 1142 1168\n",
      " 1169 1178 1181 1182 1186 1214 1230 1234 1237 1258 1282 1286 1288 1309 1313\n",
      " 1342 1356 1365 1372 1394 1402 1404 1406 1433 1437 1481 1508 1509 1517 1522\n",
      " 1545 1546 1575 1576 1583 1584 1595 1600 1616 1620 1623 1628 1629 1638 1649\n",
      " 1659 1676 1682 1685 1688 1692 1730 1745 1751 1755 1782 1788 1791 1793 1796\n",
      " 1797 1799 1807 1810 1812 1815 1835 1843 1856 1866 1884 1891 1911 1917 1926\n",
      " 1932 1958 1962 1965 1986 1990 1995 2002 2041 2047 2048 2049 2066 2068 2077\n",
      " 2079 2098 2112 2113 2128 2137 2142 2148 2155 2162 2168 2170 2200 2206 2209\n",
      " 2211 2212 2221 2234 2252 2261 2277 2291 2299 2313 2350 2353 2361 2366 2369\n",
      " 2370 2374 2384 2388 2426 2445 2448 2449 2455 2476 2486 2504 2513 2533 2539\n",
      " 2542 2565 2573 2583 2595]\n",
      "TRAIN: [   1    2    4 ..., 2603 2604 2605] TEST: [   0    3    7   21   26   63   93  100  112  126  153  160  163  164  174\n",
      "  237  246  280  281  282  292  321  325  327  329  334  343  348  350  365\n",
      "  375  387  400  404  407  419  424  428  437  447  449  451  460  470  497\n",
      "  504  550  573  577  584  586  594  595  603  604  605  606  624  627  640\n",
      "  647  650  664  671  673  680  691  694  696  698  699  709  732  736  738\n",
      "  739  741  754  780  786  800  804  827  830  834  837  845  848  851  854\n",
      "  859  869  873  902  903  929  932  941  945  950  952  975  990 1004 1006\n",
      " 1011 1028 1040 1046 1048 1090 1111 1113 1130 1131 1133 1144 1149 1159 1163\n",
      " 1177 1179 1194 1201 1202 1209 1215 1219 1221 1243 1247 1249 1250 1251 1253\n",
      " 1289 1298 1305 1306 1308 1314 1331 1333 1337 1348 1353 1369 1384 1389 1398\n",
      " 1409 1413 1416 1425 1438 1441 1443 1461 1468 1479 1480 1497 1514 1532 1541\n",
      " 1542 1551 1556 1558 1562 1566 1574 1579 1582 1596 1608 1617 1660 1667 1671\n",
      " 1672 1690 1705 1707 1722 1734 1744 1747 1769 1800 1805 1834 1838 1844 1847\n",
      " 1849 1862 1868 1879 1883 1889 1915 1924 1933 1938 1941 1963 1967 1971 1975\n",
      " 1978 1993 2003 2006 2009 2021 2040 2042 2051 2107 2136 2150 2152 2153 2154\n",
      " 2180 2182 2185 2189 2195 2215 2247 2271 2295 2304 2306 2309 2315 2331 2340\n",
      " 2347 2377 2379 2380 2385 2390 2412 2424 2427 2443 2454 2470 2473 2488 2501\n",
      " 2523 2527 2591 2593 2602]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  24   25   67   84   86   91   94   99  130  136  146  151  166  168  176\n",
      "  180  197  199  201  207  209  216  221  256  257  273  274  275  277  291\n",
      "  297  307  323  324  368  373  388  423  429  430  433  469  508  509  525\n",
      "  537  544  555  556  560  591  607  623  637  639  659  705  714  730  749\n",
      "  755  756  763  767  770  774  797  802  807  809  816  821  835  843  860\n",
      "  865  872  885  888  894  908  925  928  931  954  956  967  972  973  976\n",
      " 1007 1020 1022 1033 1053 1071 1085 1104 1107 1112 1134 1141 1152 1153 1155\n",
      " 1162 1167 1172 1176 1198 1204 1206 1207 1208 1241 1272 1278 1297 1304 1316\n",
      " 1329 1345 1346 1350 1352 1381 1392 1395 1400 1429 1434 1435 1445 1447 1466\n",
      " 1469 1470 1472 1483 1488 1531 1536 1552 1561 1563 1565 1578 1589 1605 1619\n",
      " 1634 1640 1641 1645 1653 1684 1699 1701 1718 1720 1723 1731 1740 1750 1777\n",
      " 1778 1792 1795 1822 1823 1828 1863 1871 1877 1896 1899 1904 1905 1908 1910\n",
      " 1913 1920 1925 1930 1940 1954 1956 1966 1970 1973 1987 2008 2011 2022 2023\n",
      " 2024 2036 2046 2059 2071 2076 2080 2081 2084 2089 2091 2105 2117 2120 2121\n",
      " 2135 2139 2141 2146 2159 2163 2169 2171 2176 2177 2187 2197 2217 2222 2237\n",
      " 2251 2257 2260 2280 2282 2292 2322 2334 2335 2338 2339 2344 2362 2371 2383\n",
      " 2389 2415 2418 2425 2431 2435 2446 2483 2490 2494 2496 2517 2524 2554 2558\n",
      " 2568 2577 2578 2584 2587]\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that ShuffleSplit has selected a random set of observations from the index of our data set for each fold of our cross-validation. Let's look at the size of our training and test set for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2346 TEST: 260\n",
      "TRAIN: 2346 TEST: 260\n",
      "TRAIN: 2346 TEST: 260\n",
      "TRAIN: 2346 TEST: 260\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try using KFold to train and test our model on 10 different subsets of our data. Below we set our cross-validator as 'cv'. We then loop through the various splits in our data that cv creates and use it to make our training and test sets. We then use our training set to fit a Logistic Regression model and generate predictions from our test set, which we compare to the actual outcomes we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 56.9\n",
      "Precision: 55.8\n",
      "Precision: 60.4\n",
      "Precision: 63.5\n",
      "Precision: 55.4\n",
      "Precision: 52.8\n",
      "Precision: 56.9\n",
      "Precision: 57.1\n",
      "Precision: 65.8\n",
      "Precision: 41.0\n"
     ]
    }
   ],
   "source": [
    "## Define function\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "## Create for-loop\n",
    "for train_index, test_index in cv.split(data):\n",
    "\n",
    "    ## Define training and test sets\n",
    "    X_train = data.loc[train_index].drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_train = data.loc[train_index]['activity']\n",
    "    X_test = data.loc[test_index].drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_test = data.loc[test_index]['activity']\n",
    "        \n",
    "    ## Fit model\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ## Generate predictions\n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    ## Compare to actual outcomes and return precision\n",
    "    print('Precision: '+str(100 * round(precision_score(y_test, predicted),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that, for the most part, about 50 to 60 percent of the inspections our model predicts will lead our inspectors to rat burrows actually do. This is a modest improvement over our inspectors' current performance in the field. Based on these results, if we used our models to determine which locations our inspectors go to in the field, we'd probably see a 10 to 20 point increase in their likelihood of finding rat burrows.\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Try running the k-fold cross-validation a few times with the same random state. Then try running it a few times with different random states. How do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  33   53  117  156  192  195  227  252  271  289  314  317  418  422  436\n",
      "  438  443  478  518  547  562  567  581  616  618  704  711  722  728  745\n",
      "  748  764  778  795  868  880  899  913  921  927  982  988  998 1012 1013\n",
      " 1052 1059 1078 1079 1096 1100 1101 1109 1192 1213 1263 1285 1323 1326 1327\n",
      " 1396 1418 1432 1452 1507 1570 1577 1580 1592 1622 1627 1680 1681 1686 1708\n",
      " 1710 1719 1729 1735 1757 1762 1763 1765 1803 1818 1821 1832 1853 1873 1874\n",
      " 1921 1929 1939 1943 1961 1968 1969 1972 1974 1989 1997 2019 2027 2033 2074\n",
      " 2087 2090 2134 2138 2156 2186 2191 2225 2274 2289 2312 2316 2317 2343 2358\n",
      " 2386 2416 2419 2429 2468 2498 2510 2543 2550 2555 2597]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   9   22   27   70   92  104  109  121  135  137  182  196  215  217  224\n",
      "  259  276  326  333  351  398  399  427  452  465  480  482  489  563  569\n",
      "  578  597  609  619  674  682  686  700  710  720  743  746  817  831  855\n",
      "  878  916  933  961  962  983 1000 1018 1023 1032 1036 1051 1106 1108 1147\n",
      " 1187 1264 1287 1300 1371 1421 1467 1484 1515 1520 1544 1568 1585 1588 1590\n",
      " 1597 1656 1657 1668 1689 1732 1761 1780 1783 1790 1798 1814 1816 1820 1827\n",
      " 1836 1895 1898 1901 1902 1927 1979 1982 1998 2030 2055 2085 2115 2123 2151\n",
      " 2161 2183 2230 2254 2258 2281 2302 2345 2346 2359 2378 2387 2398 2400 2410\n",
      " 2417 2450 2499 2502 2520 2544 2564 2567 2579 2585 2596]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  14   37   41   57   69  132  148  191  204  234  296  302  303  311  320\n",
      "  330  353  361  402  405  446  487  526  527  536  558  565  582  610  615\n",
      "  621  638  652  653  666  667  672  676  703  713  758  776  828  840  876\n",
      "  891  909  914  924  935  997 1002 1041 1068 1070 1091 1093 1110 1122 1146\n",
      " 1150 1161 1173 1185 1197 1199 1211 1222 1239 1244 1303 1334 1362 1366 1405\n",
      " 1412 1424 1465 1486 1493 1512 1525 1539 1553 1573 1601 1646 1663 1712 1714\n",
      " 1768 1770 1774 1781 1813 1851 1937 1946 1949 1988 2001 2007 2038 2067 2069\n",
      " 2088 2116 2201 2207 2245 2293 2297 2321 2354 2376 2381 2395 2399 2405 2409\n",
      " 2482 2491 2497 2508 2512 2515 2531 2556 2561 2566 2588]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   4   10   23   39   76   98  113  124  145  162  179  232  245  248  251\n",
      "  357  379  385  390  414  425  440  454  457  477  486  501  543  634  687\n",
      "  688  692  702  708  712  715  716  789  812  838  847  852  897  898  905\n",
      "  926  966  989 1003 1025 1118 1138 1193 1210 1228 1231 1232 1242 1270 1292\n",
      " 1294 1332 1377 1380 1414 1426 1449 1487 1496 1504 1506 1535 1543 1548 1549\n",
      " 1555 1594 1599 1625 1637 1642 1664 1665 1675 1678 1702 1703 1727 1728 1809\n",
      " 1824 1825 1826 1839 1845 1852 1859 1885 1928 1952 1955 2013 2037 2045 2054\n",
      " 2073 2095 2131 2143 2149 2175 2188 2213 2229 2233 2246 2262 2266 2272 2276\n",
      " 2288 2333 2341 2352 2356 2442 2526 2559 2562 2592 2594]\n",
      "TRAIN: [   0    2    3 ..., 2603 2604 2605] TEST: [   1   11   17   40   48   52   58  107  125  133  149  157  189  194  206\n",
      "  220  229  264  286  305  322  384  391  411  442  453  505  530  538  557\n",
      "  568  575  579  587  602  641  670  723  731  768  792  805  841  850  886\n",
      "  892  900  906  936  953  963  996 1009 1015 1027 1063 1103 1116 1139 1140\n",
      " 1160 1190 1236 1273 1283 1317 1322 1328 1357 1367 1386 1393 1411 1448 1459\n",
      " 1492 1503 1521 1533 1540 1547 1567 1569 1602 1632 1677 1697 1713 1753 1754\n",
      " 1802 1819 1829 1840 1855 1861 1869 1947 1983 1985 1999 2015 2026 2099 2109\n",
      " 2124 2165 2190 2198 2208 2223 2235 2249 2256 2279 2290 2308 2326 2336 2372\n",
      " 2407 2414 2430 2462 2471 2477 2480 2495 2511 2549 2576]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   6   18   30   43   47  161  173  175  200  249  254  283  300  342  386\n",
      "  392  444  458  459  461  488  503  517  529  535  570  574  596  646  648\n",
      "  651  657  661  665  684  727  757  762  895  918  949  951  995 1005 1017\n",
      " 1042 1055 1058 1073 1081 1098 1114 1126 1127 1157 1174 1188 1203 1205 1226\n",
      " 1246 1256 1267 1271 1295 1302 1370 1373 1422 1428 1431 1450 1471 1513 1523\n",
      " 1528 1598 1621 1631 1643 1673 1704 1715 1724 1725 1726 1736 1737 1773 1854\n",
      " 1864 1867 1872 1876 1897 1900 1931 1942 1945 1964 1996 2012 2039 2052 2072\n",
      " 2083 2097 2144 2145 2238 2240 2284 2285 2300 2320 2355 2357 2367 2382 2396\n",
      " 2422 2432 2467 2500 2518 2525 2548 2560 2569 2582 2601]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  31   64   77   80   85   87  106  141  169  202  233  240  250  253  262\n",
      "  270  279  287  299  309  359  369  381  393  475  483  485  491  512  521\n",
      "  532  553  655  663  721  733  766  794  799  810  815  818  820  829  842\n",
      "  861  882  890  896  910  923  938  944  958  964  971  979  980  981  991\n",
      " 1010 1038 1047 1049 1056 1074 1083 1087 1097 1119 1151 1183 1217 1240 1254\n",
      " 1255 1341 1347 1368 1390 1397 1458 1490 1491 1559 1560 1571 1581 1593 1610\n",
      " 1648 1674 1709 1759 1779 1789 1808 1865 1892 1923 1935 1980 2020 2044 2050\n",
      " 2053 2061 2122 2129 2133 2140 2178 2204 2219 2259 2263 2296 2311 2319 2351\n",
      " 2360 2402 2441 2444 2447 2469 2507 2522 2536 2571]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  15   34   61   72  118  139  144  187  223  260  267  294  295  298  310\n",
      "  360  376  415  474  502  506  516  519  522  546  564  572  600  620  629\n",
      "  633  636  643  654  701  735  775  781  791  793  806  825  832  836  883\n",
      "  917  937  946  977  986 1043 1045 1077 1099 1136 1148 1170 1180 1212 1279\n",
      " 1301 1325 1330 1339 1355 1374 1385 1388 1399 1417 1453 1456 1474 1485 1516\n",
      " 1587 1603 1611 1647 1694 1696 1758 1760 1786 1787 1801 1817 1860 1875 1903\n",
      " 1907 1916 1919 1948 1994 2032 2057 2063 2064 2075 2078 2092 2158 2164 2173\n",
      " 2203 2210 2273 2298 2305 2328 2391 2392 2394 2397 2401 2406 2408 2413 2457\n",
      " 2458 2478 2479 2493 2509 2519 2538 2574 2575 2581]\n",
      "TRAIN: [   0    1    2 ..., 2602 2604 2605] TEST: [   5   19   36   42   54   55   66   82  102  122  140  142  152  159  185\n",
      "  190  214  219  272  278  315  318  335  383  408  434  468  473  479  511\n",
      "  534  551  583  598  689  717  718  724  726  750  796  811  813  866  893\n",
      "  974  978  985  987 1008 1029 1069 1076 1082 1129 1137 1165 1175 1235 1275\n",
      " 1277 1280 1284 1293 1338 1343 1358 1376 1379 1427 1457 1460 1462 1489 1500\n",
      " 1530 1538 1550 1606 1633 1666 1679 1742 1775 1785 1858 1878 1890 1909 1950\n",
      " 1957 1976 2000 2005 2018 2025 2028 2035 2056 2108 2166 2181 2242 2244 2248\n",
      " 2264 2265 2267 2269 2278 2283 2310 2324 2349 2434 2452 2453 2463 2465 2484\n",
      " 2487 2492 2505 2506 2528 2552 2570 2598 2599 2603]\n",
      "TRAIN: [   0    1    3 ..., 2603 2604 2605] TEST: [   2   45   65   73   89  108  154  170  171  177  178  184  198  203  210\n",
      "  268  308  312  319  340  341  347  364  378  412  416  467  481  498  539\n",
      "  561  590  632  644  658  740  744  759  760  772  773  785  801  823  839\n",
      "  856  863  875  930  955  965  992 1001 1021 1026 1050 1080 1117 1132 1135\n",
      " 1145 1154 1166 1195 1216 1225 1257 1259 1261 1266 1349 1359 1363 1378 1387\n",
      " 1423 1454 1473 1478 1482 1499 1518 1526 1537 1612 1615 1635 1661 1717 1748\n",
      " 1749 1752 1764 1846 1848 1886 1914 1934 1960 2010 2017 2031 2034 2058 2093\n",
      " 2100 2102 2119 2160 2172 2194 2202 2218 2228 2236 2253 2270 2303 2325 2364\n",
      " 2365 2393 2421 2438 2456 2460 2475 2485 2514 2532]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  13   16   56   68   71   96  114  147  211  218  231  241  258  339  354\n",
      "  355  362  367  372  377  406  420  432  435  456  500  513  520  542  549\n",
      "  552  554  576  589  593  642  649  677  679  693  706  725  769  788  803\n",
      "  814  862  881  884  889  939  940  942  948  957  960  970  984  994 1120\n",
      " 1128 1143 1164 1248 1252 1265 1299 1319 1354 1403 1436 1444 1451 1455 1463\n",
      " 1464 1476 1505 1519 1650 1652 1655 1670 1721 1741 1746 1766 1767 1771 1794\n",
      " 1806 1837 1850 1870 1888 1922 1953 1984 1992 2016 2060 2070 2096 2111 2126\n",
      " 2157 2167 2174 2193 2216 2224 2231 2250 2255 2301 2307 2323 2330 2403 2404\n",
      " 2423 2436 2440 2461 2466 2516 2557 2572 2589 2600]\n",
      "TRAIN: [   0    1    2 ..., 2602 2603 2605] TEST: [   8   29   32   35   49   60   75  110  134  155  186  212  226  243  244\n",
      "  247  265  285  293  304  336  358  401  426  431  445  450  455  464  471\n",
      "  494  499  524  533  540  548  580  585  611  631  668  678  737  752  782\n",
      "  871  887  943  999 1014 1016 1031 1044 1060 1064 1065 1075 1105 1123 1124\n",
      " 1171 1184 1189 1200 1218 1227 1229 1245 1310 1312 1315 1336 1340 1361 1407\n",
      " 1446 1495 1498 1501 1511 1554 1572 1586 1591 1604 1607 1626 1644 1654 1662\n",
      " 1669 1683 1698 1716 1738 1739 1776 1880 1893 1894 1936 1944 1951 1981 2029\n",
      " 2043 2062 2065 2082 2125 2130 2132 2192 2196 2205 2243 2268 2314 2337 2363\n",
      " 2368 2481 2537 2540 2541 2547 2563 2580 2586 2604]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  44   78   83  103  129  131  165  205  213  263  269  313  328  338  349\n",
      "  363  371  395  439  448  466  472  476  507  510  528  625  635  645  656\n",
      "  685  695  697  761  783  784  808  819  826  833  844  846  849  853  858\n",
      "  879  904  907  911  919  947  969 1019 1034 1039 1057 1084 1088 1094 1102\n",
      " 1115 1196 1260 1269 1276 1281 1307 1311 1324 1335 1344 1351 1360 1382 1383\n",
      " 1401 1415 1419 1420 1430 1439 1440 1442 1494 1510 1524 1529 1564 1609 1614\n",
      " 1618 1624 1639 1651 1687 1691 1695 1733 1772 1784 1811 1830 1831 1841 1912\n",
      " 1918 1959 1977 2103 2104 2106 2147 2184 2214 2227 2239 2286 2318 2411 2420\n",
      " 2428 2439 2459 2472 2474 2489 2503 2534 2535 2546]\n",
      "TRAIN: [   0    1    2 ..., 2602 2603 2604] TEST: [  38   51   59   81   88   97  111  119  123  183  188  238  239  261  316\n",
      "  356  374  382  397  410  463  484  490  493  514  541  571  599  601  608\n",
      "  613  660  681  729  751  771  779  867  874  934  993 1035 1067 1089 1121\n",
      " 1125 1156 1158 1191 1220 1223 1224 1233 1238 1262 1268 1274 1290 1291 1296\n",
      " 1318 1320 1321 1364 1375 1391 1408 1410 1475 1477 1502 1527 1534 1557 1613\n",
      " 1630 1636 1658 1693 1700 1706 1711 1743 1756 1804 1833 1842 1857 1881 1882\n",
      " 1887 1906 1991 2004 2014 2086 2094 2101 2110 2114 2118 2127 2179 2199 2220\n",
      " 2226 2232 2241 2275 2287 2294 2327 2329 2332 2342 2348 2373 2375 2433 2437\n",
      " 2451 2464 2521 2529 2530 2545 2551 2553 2590 2605]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  62   79   95  101  120  127  143  150  158  208  222  228  230  236  255\n",
      "  290  332  345  346  352  366  380  394  396  403  413  417  492  495  496\n",
      "  523  531  559  566  588  592  626  662  675  719  742  765  787  790  798\n",
      "  824  864  877  901  912  959 1024 1037 1054 1061 1086 1095 1168 1169 1178\n",
      " 1182 1214 1230 1237 1288 1342 1372 1437 1481 1509 1576 1583 1595 1616 1620\n",
      " 1628 1629 1649 1685 1745 1751 1782 1788 1791 1796 1797 1810 1815 1856 1866\n",
      " 1884 1962 1965 1995 2041 2047 2048 2049 2066 2068 2077 2079 2098 2112 2113\n",
      " 2128 2142 2155 2162 2206 2209 2212 2234 2252 2261 2277 2299 2313 2350 2353\n",
      " 2366 2370 2374 2448 2449 2486 2504 2533 2539 2542]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  12   20   28   46   50   74   90  105  115  116  128  138  167  172  181\n",
      "  193  225  235  242  266  284  288  301  306  331  337  344  370  389  409\n",
      "  421  441  462  515  545  612  614  617  622  628  630  669  683  690  707\n",
      "  734  747  753  777  822  857  870  915  920  922  968 1030 1062 1066 1072\n",
      " 1092 1142 1181 1186 1234 1258 1282 1286 1309 1313 1356 1365 1394 1402 1404\n",
      " 1406 1433 1508 1517 1522 1545 1546 1575 1584 1600 1623 1638 1659 1676 1682\n",
      " 1688 1692 1730 1755 1793 1799 1807 1812 1835 1843 1891 1911 1917 1926 1932\n",
      " 1958 1986 1990 2002 2137 2148 2168 2170 2200 2211 2221 2291 2361 2369 2384\n",
      " 2388 2426 2445 2455 2476 2513 2565 2573 2583 2595]\n",
      "TRAIN: [   1    2    3 ..., 2603 2604 2605] TEST: [   0   21   93  100  174  246  282  321  325  327  334  343  350  375  387\n",
      "  404  419  424  428  447  451  460  497  594  604  605  624  627  640  664\n",
      "  671  673  691  694  696  699  709  738  741  754  780  827  830  834  845\n",
      "  848  869  902  950  975 1011 1046 1090 1113 1131 1133 1149 1159 1177 1179\n",
      " 1194 1201 1202 1209 1221 1243 1247 1253 1305 1333 1337 1353 1369 1389 1398\n",
      " 1409 1413 1416 1425 1438 1468 1480 1542 1551 1562 1574 1582 1608 1705 1707\n",
      " 1722 1734 1747 1769 1838 1849 1879 1889 1924 1933 1938 1941 1975 1993 2003\n",
      " 2006 2021 2042 2051 2136 2150 2152 2153 2180 2189 2215 2247 2271 2295 2304\n",
      " 2306 2315 2340 2380 2424 2427 2454 2591 2593 2602]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   3    7   26   63  112  126  153  160  163  164  237  280  281  292  329\n",
      "  348  365  400  407  437  449  470  504  550  573  577  584  586  595  603\n",
      "  606  647  650  680  698  732  736  739  786  800  804  837  851  854  859\n",
      "  873  903  929  932  941  945  952  990 1004 1006 1028 1040 1048 1111 1130\n",
      " 1144 1163 1215 1219 1249 1250 1251 1289 1298 1306 1308 1314 1331 1348 1384\n",
      " 1441 1443 1461 1479 1497 1514 1532 1541 1556 1558 1566 1579 1596 1617 1660\n",
      " 1667 1671 1672 1690 1744 1800 1805 1834 1844 1847 1862 1868 1883 1915 1963\n",
      " 1967 1971 1978 2009 2040 2107 2154 2182 2185 2195 2309 2331 2347 2377 2379\n",
      " 2385 2390 2412 2443 2470 2473 2488 2501 2523 2527]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  24   25   67  136  146  166  168  176  180  197  199  207  216  221  274\n",
      "  291  297  323  429  433  469  508  509  525  556  591  623  730  749  767\n",
      "  807  809  821  843  860  865  885  894  908  925  928  956  967 1007 1020\n",
      " 1022 1053 1085 1107 1112 1134 1141 1153 1162 1172 1176 1241 1272 1278 1297\n",
      " 1346 1350 1381 1392 1400 1429 1445 1447 1469 1470 1483 1488 1531 1552 1561\n",
      " 1563 1565 1619 1640 1645 1718 1720 1740 1750 1777 1792 1795 1822 1823 1863\n",
      " 1896 1908 1910 1913 1970 1973 2011 2022 2081 2084 2089 2091 2117 2121 2139\n",
      " 2141 2159 2169 2177 2187 2197 2237 2257 2260 2280 2282 2322 2335 2338 2371\n",
      " 2389 2415 2483 2490 2494 2517 2524 2554 2578 2584]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  84   86   91   94   99  130  151  201  209  256  257  273  275  277  307\n",
      "  324  368  373  388  423  430  537  544  555  560  607  637  639  659  705\n",
      "  714  755  756  763  770  774  797  802  816  835  872  888  931  954  972\n",
      "  973  976 1033 1071 1104 1152 1155 1167 1198 1204 1206 1207 1208 1304 1316\n",
      " 1329 1345 1352 1395 1434 1435 1466 1472 1536 1578 1589 1605 1634 1641 1653\n",
      " 1684 1699 1701 1723 1731 1778 1828 1871 1877 1899 1904 1905 1920 1925 1930\n",
      " 1940 1954 1956 1966 1987 2008 2023 2024 2036 2046 2059 2071 2076 2080 2105\n",
      " 2120 2135 2146 2163 2171 2176 2217 2222 2251 2292 2334 2339 2344 2362 2383\n",
      " 2418 2425 2431 2435 2446 2496 2558 2568 2577 2587]\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=20, shuffle=True, random_state=0)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2475 TEST: 131\n",
      "TRAIN: 2475 TEST: 131\n",
      "TRAIN: 2475 TEST: 131\n",
      "TRAIN: 2475 TEST: 131\n",
      "TRAIN: 2475 TEST: 131\n",
      "TRAIN: 2475 TEST: 131\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n",
      "TRAIN: 2476 TEST: 130\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=20, shuffle=True, random_state=0)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 56.9\n",
      "Precision: 55.8\n",
      "Precision: 60.4\n",
      "Precision: 63.5\n",
      "Precision: 55.4\n",
      "Precision: 52.8\n",
      "Precision: 56.9\n",
      "Precision: 57.1\n",
      "Precision: 65.8\n",
      "Precision: 41.0\n"
     ]
    }
   ],
   "source": [
    "## Define function\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "## Create for-loop\n",
    "for train_index, test_index in cv.split(data):\n",
    "\n",
    "    ## Define training and test sets\n",
    "    X_train = data.loc[train_index].drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_train = data.loc[train_index]['activity']\n",
    "    X_test = data.loc[test_index].drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_test = data.loc[test_index]['activity']\n",
    "        \n",
    "    ## Fit model\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ## Generate predictions\n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    ## Compare to actual outcomes and return precision\n",
    "    print('Precision: '+str(100 * round(precision_score(y_test, predicted),3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  10   15   23   29   31   54   63   65   71  115  117  121  130  134  147\n",
      "  151  168  171  176  182  203  229  231  249  257  258  260  264  269  299\n",
      "  302  309  316  334  339  342  345  349  353  356  362  376  381  384  386\n",
      "  411  416  433  435  459  469  472  483  490  496  498  510  515  519  526\n",
      "  530  532  536  551  560  583  585  589  606  635  642  647  650  654  679\n",
      "  696  709  730  740  752  786  791  801  805  811  823  843  853  881  886\n",
      "  888  898  899  914  917  918  919  924  932  947  950  952  957  978  980\n",
      "  981  983  996 1006 1022 1023 1046 1054 1062 1067 1079 1127 1134 1135 1136\n",
      " 1148 1158 1165 1166 1181 1185 1187 1197 1216 1224 1225 1243 1249 1254 1262\n",
      " 1265 1270 1306 1313 1329 1339 1352 1356 1360 1369 1378 1380 1383 1389 1394\n",
      " 1396 1410 1421 1436 1449 1453 1474 1482 1492 1499 1514 1517 1529 1540 1547\n",
      " 1555 1568 1590 1591 1603 1656 1674 1692 1697 1704 1727 1737 1739 1741 1743\n",
      " 1762 1775 1776 1783 1788 1799 1810 1811 1836 1847 1857 1882 1883 1886 1888\n",
      " 1891 1897 1903 1917 1941 1966 1969 1970 1974 2007 2030 2043 2044 2075 2092\n",
      " 2105 2130 2140 2155 2168 2190 2201 2202 2207 2208 2217 2224 2226 2239 2262\n",
      " 2264 2282 2288 2293 2302 2306 2339 2345 2347 2348 2376 2395 2397 2400 2409\n",
      " 2413 2415 2436 2456 2484 2490 2501 2507 2512 2513 2518 2519 2522 2540 2554\n",
      " 2558 2566 2573 2583 2598 2599]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  16   27   28   36   42   57   75   85  107  114  116  124  141  174  189\n",
      "  221  225  228  232  238  247  259  270  272  275  289  290  291  318  322\n",
      "  328  338  343  355  382  399  403  408  426  428  434  440  442  450  452\n",
      "  463  474  489  500  503  533  562  595  605  610  616  626  627  631  632\n",
      "  636  646  658  685  686  688  699  704  706  708  724  726  731  733  737\n",
      "  742  743  746  762  771  789  821  838  841  851  920  925  927  931  934\n",
      "  936  954  999 1010 1011 1013 1025 1029 1036 1044 1048 1058 1059 1080 1097\n",
      " 1107 1132 1140 1146 1153 1156 1160 1164 1169 1180 1198 1205 1211 1212 1215\n",
      " 1231 1235 1246 1252 1260 1266 1267 1268 1282 1283 1286 1302 1304 1312 1318\n",
      " 1323 1327 1335 1338 1344 1358 1359 1371 1372 1390 1398 1417 1440 1462 1463\n",
      " 1471 1494 1511 1515 1520 1523 1541 1554 1567 1570 1576 1581 1584 1586 1587\n",
      " 1601 1618 1631 1638 1668 1670 1678 1686 1688 1705 1713 1718 1726 1742 1755\n",
      " 1793 1796 1798 1800 1815 1863 1864 1880 1895 1923 1925 1953 1956 1963 1973\n",
      " 1975 1989 1992 1995 2000 2006 2008 2020 2023 2039 2047 2051 2058 2064 2070\n",
      " 2091 2107 2129 2148 2183 2193 2233 2241 2249 2253 2254 2255 2256 2261 2276\n",
      " 2291 2292 2301 2307 2312 2319 2333 2357 2359 2368 2369 2379 2386 2387 2405\n",
      " 2406 2414 2432 2440 2446 2472 2474 2488 2491 2502 2509 2514 2529 2538 2557\n",
      " 2562 2564 2579 2585 2588 2596]\n",
      "TRAIN: [   2    3    4 ..., 2602 2603 2604] TEST: [   0    1    5   19   30   44   48   55   84   91   95  103  104  106  118\n",
      "  152  153  159  173  177  190  208  211  223  244  250  251  277  288  301\n",
      "  317  320  331  336  341  344  348  352  361  363  368  371  374  383  385\n",
      "  391  398  401  405  413  415  423  425  432  438  443  480  487  493  499\n",
      "  511  521  529  534  542  548  549  552  567  568  569  582  584  588  591\n",
      "  624  641  652  656  661  669  687  701  719  723  772  774  775  784  795\n",
      "  798  815  828  830  834  836  859  863  880  883  887  891  900  903  904\n",
      "  913  930  935  937  940  945  946  965  967  969  970  971  974 1019 1026\n",
      " 1042 1047 1050 1052 1053 1055 1076 1083 1086 1087 1089 1104 1105 1121 1122\n",
      " 1131 1139 1149 1176 1294 1297 1299 1301 1322 1325 1330 1342 1354 1370 1375\n",
      " 1403 1423 1435 1447 1473 1490 1538 1542 1552 1556 1562 1573 1579 1588 1589\n",
      " 1592 1599 1611 1614 1625 1626 1636 1640 1642 1649 1662 1664 1665 1671 1703\n",
      " 1706 1711 1715 1717 1724 1729 1735 1736 1746 1759 1778 1785 1786 1809 1828\n",
      " 1831 1834 1840 1849 1872 1911 1921 1928 1958 1959 1965 1983 1993 2001 2004\n",
      " 2037 2078 2100 2109 2119 2131 2151 2153 2160 2162 2169 2173 2177 2222 2259\n",
      " 2266 2277 2278 2284 2285 2308 2311 2316 2318 2331 2336 2340 2370 2378 2381\n",
      " 2392 2393 2402 2404 2417 2443 2476 2489 2494 2504 2516 2517 2524 2531 2534\n",
      " 2542 2565 2568 2570 2594 2605]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   4   12   51   56   66   70   90   93   97  102  105  110  111  137  138\n",
      "  139  149  165  170  183  185  191  193  198  201  205  207  230  237  241\n",
      "  242  248  256  268  287  310  321  326  329  354  365  367  372  389  400\n",
      "  402  414  427  455  494  497  509  528  531  538  540  545  555  561  571\n",
      "  592  598  599  613  625  629  638  644  648  657  663  670  682  683  710\n",
      "  718  725  728  729  739  741  747  751  753  773  777  781  785  807  808\n",
      "  819  825  845  858  860  864  878  885  906  912  939  941  951  958  962\n",
      "  973  995 1007 1012 1017 1043 1064 1065 1069 1099 1111 1112 1113 1117 1142\n",
      " 1157 1161 1162 1178 1191 1193 1199 1208 1240 1247 1263 1271 1273 1276 1290\n",
      " 1298 1308 1309 1348 1367 1400 1431 1434 1444 1446 1450 1451 1456 1465 1503\n",
      " 1530 1532 1553 1557 1582 1598 1617 1653 1658 1660 1676 1677 1689 1690 1696\n",
      " 1702 1710 1714 1721 1732 1734 1740 1745 1763 1770 1773 1780 1784 1789 1795\n",
      " 1801 1806 1807 1816 1818 1844 1861 1869 1881 1889 1914 1931 1935 1938 1942\n",
      " 1967 1994 2010 2011 2019 2028 2035 2041 2042 2054 2057 2062 2073 2076 2080\n",
      " 2081 2089 2102 2110 2114 2124 2159 2167 2185 2187 2192 2198 2214 2221 2228\n",
      " 2229 2258 2275 2297 2315 2324 2330 2337 2353 2354 2360 2366 2403 2411 2429\n",
      " 2441 2444 2445 2451 2458 2465 2471 2477 2480 2492 2499 2500 2511 2535 2536\n",
      " 2546 2551 2561 2567 2575 2584]\n",
      "TRAIN: [   0    1    2 ..., 2602 2603 2605] TEST: [   8    9   22   40   41   43   45   47   49   64   67   69   78   89   96\n",
      "  101  108  142  143  146  154  157  161  164  169  178  188  200  204  209\n",
      "  210  216  222  261  265  267  285  298  308  330  347  380  390  395  396\n",
      "  420  431  447  464  471  479  482  486  492  517  525  546  565  573  581\n",
      "  586  602  608  623  630  666  667  671  697  711  712  738  755  761  766\n",
      "  767  790  792  816  835  837  842  868  873  874  901  933  943  960  968\n",
      "  977  984  988  993 1005 1016 1020 1039 1049 1066 1070 1073 1088 1092 1125\n",
      " 1128 1133 1143 1147 1151 1167 1172 1188 1207 1222 1228 1237 1248 1264 1280\n",
      " 1291 1292 1293 1314 1321 1345 1361 1364 1365 1381 1382 1384 1399 1406 1407\n",
      " 1414 1418 1428 1439 1442 1461 1470 1472 1479 1480 1487 1493 1498 1508 1527\n",
      " 1535 1546 1561 1566 1572 1577 1612 1622 1627 1637 1657 1661 1666 1699 1709\n",
      " 1725 1752 1767 1769 1772 1794 1804 1805 1808 1812 1825 1826 1827 1833 1853\n",
      " 1855 1862 1874 1876 1893 1905 1910 1912 1919 1934 1950 1952 1978 1980 1986\n",
      " 1990 1999 2009 2017 2025 2029 2032 2033 2034 2046 2060 2069 2085 2088 2090\n",
      " 2094 2099 2101 2115 2117 2136 2144 2145 2157 2178 2181 2182 2203 2204 2209\n",
      " 2225 2245 2257 2273 2279 2287 2309 2310 2313 2322 2327 2328 2338 2346 2349\n",
      " 2352 2371 2416 2419 2425 2426 2428 2449 2466 2481 2498 2503 2510 2515 2528\n",
      " 2553 2555 2582 2600 2601 2604]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  24   25   72   80   82   88   92  122  133  140  150  155  162  192  195\n",
      "  202  213  220  224  239  263  266  286  295  306  312  315  323  325  360\n",
      "  370  373  377  430  436  444  457  461  466  467  475  501  508  516  520\n",
      "  537  554  557  559  570  575  590  593  611  614  621  622  675  676  677\n",
      "  691  692  702  713  714  717  721  722  734  749  764  779  814  820  822\n",
      "  827  832  847  852  855  857  865  875  876  877  892  902  905  908  911\n",
      "  916  944  948  949  953  964  972  976  979  982  987  991 1001 1004 1021\n",
      " 1035 1063 1068 1071 1072 1078 1085 1090 1093 1115 1116 1119 1126 1129 1138\n",
      " 1159 1170 1174 1175 1182 1184 1186 1202 1219 1220 1229 1230 1238 1241 1244\n",
      " 1245 1272 1275 1277 1279 1324 1332 1351 1366 1377 1402 1404 1424 1426 1429\n",
      " 1432 1441 1455 1478 1491 1505 1512 1516 1521 1533 1536 1539 1545 1551 1558\n",
      " 1583 1594 1596 1604 1616 1620 1630 1633 1634 1644 1645 1654 1663 1669 1693\n",
      " 1700 1749 1756 1761 1766 1782 1832 1841 1842 1879 1906 1909 1913 1936 1943\n",
      " 1949 1964 1971 1977 1985 1988 2016 2021 2031 2061 2126 2146 2152 2164 2165\n",
      " 2179 2184 2194 2197 2199 2205 2213 2227 2236 2238 2248 2251 2252 2271 2272\n",
      " 2286 2295 2320 2344 2350 2355 2375 2377 2385 2391 2398 2408 2421 2423 2437\n",
      " 2438 2447 2450 2459 2460 2461 2470 2482 2493 2497 2537 2539 2547 2552 2556\n",
      " 2559 2576 2578 2586 2589 2590]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [  32   39   46   50   52   60   61   68   73   74   81   94   98  113  125\n",
      "  126  136  156  160  163  166  167  175  179  196  197  218  219  240  245\n",
      "  253  254  276  279  294  296  314  332  335  359  387  392  404  417  419\n",
      "  424  445  454  462  484  504  522  527  553  558  574  576  577  580  587\n",
      "  601  619  634  637  645  680  689  695  707  716  732  757  769  770  778\n",
      "  800  824  844  856  861  867  869  882  890  893  896  910  922  929  963\n",
      "  975  985  989 1003 1014 1024 1027 1030 1045 1056 1060 1075 1096 1109 1130\n",
      " 1141 1144 1145 1168 1171 1173 1179 1192 1195 1204 1213 1221 1223 1242 1251\n",
      " 1255 1257 1259 1281 1284 1288 1295 1296 1300 1303 1305 1317 1337 1343 1349\n",
      " 1363 1386 1387 1408 1411 1419 1420 1452 1460 1466 1469 1481 1483 1485 1495\n",
      " 1497 1500 1504 1524 1534 1560 1563 1564 1575 1580 1606 1615 1629 1632 1641\n",
      " 1655 1684 1685 1694 1695 1707 1722 1728 1730 1733 1748 1750 1757 1774 1790\n",
      " 1792 1802 1819 1824 1830 1846 1850 1851 1854 1856 1860 1866 1867 1898 1915\n",
      " 1918 1920 1922 1927 1948 1984 1996 1997 2005 2014 2022 2050 2052 2056 2066\n",
      " 2072 2079 2093 2095 2096 2125 2127 2133 2138 2147 2149 2156 2158 2172 2196\n",
      " 2200 2219 2232 2265 2280 2300 2303 2305 2314 2325 2326 2329 2351 2356 2361\n",
      " 2384 2394 2396 2401 2407 2412 2422 2452 2453 2455 2462 2468 2473 2496 2505\n",
      " 2506 2544 2545 2591 2593]\n",
      "TRAIN: [   0    1    2 ..., 2602 2604 2605] TEST: [   3   26   33   58   59   76   99  100  109  123  132  180  181  184  199\n",
      "  206  217  227  233  235  246  252  255  271  278  281  282  305  307  311\n",
      "  319  337  358  366  378  379  388  393  406  412  422  441  448  449  460\n",
      "  476  512  524  535  541  543  547  564  566  572  578  596  607  609  615\n",
      "  620  639  640  664  665  672  673  684  703  715  727  756  759  760  776\n",
      "  780  793  797  799  810  812  831  848  850  871  884  907  921  923  928\n",
      "  955  956  959  966  986 1002 1015 1031 1034 1037 1041 1051 1095 1098 1110\n",
      " 1114 1124 1150 1177 1183 1190 1217 1226 1227 1232 1250 1253 1258 1278 1287\n",
      " 1289 1311 1326 1334 1336 1341 1346 1353 1355 1388 1391 1397 1401 1405 1409\n",
      " 1416 1437 1448 1457 1467 1486 1489 1506 1507 1513 1518 1525 1526 1528 1537\n",
      " 1559 1565 1571 1574 1600 1605 1607 1610 1621 1623 1639 1647 1651 1687 1698\n",
      " 1708 1720 1723 1731 1744 1753 1754 1764 1771 1779 1781 1803 1820 1823 1848\n",
      " 1870 1873 1887 1896 1902 1904 1937 1940 1944 1945 1946 1954 1955 1957 1962\n",
      " 1976 1982 1987 2013 2026 2038 2067 2071 2077 2098 2104 2112 2120 2121 2123\n",
      " 2132 2139 2154 2171 2175 2188 2206 2210 2211 2215 2223 2242 2243 2246 2247\n",
      " 2250 2267 2281 2298 2304 2317 2323 2335 2364 2373 2374 2380 2382 2383 2399\n",
      " 2420 2430 2435 2439 2442 2457 2475 2479 2483 2485 2486 2530 2532 2543 2549\n",
      " 2563 2569 2581 2597 2603]\n",
      "TRAIN: [   0    1    2 ..., 2603 2604 2605] TEST: [   6    7   14   18   20   21   34   35   37   62   77   86  112  120  127\n",
      "  128  129  158  186  187  194  236  284  292  293  297  300  327  333  340\n",
      "  351  369  375  397  410  418  421  453  456  458  470  478  481  485  491\n",
      "  505  507  513  518  523  539  544  563  579  594  600  603  604  633  649\n",
      "  651  662  678  681  690  693  698  700  720  735  744  745  748  750  787\n",
      "  794  817  826  840  846  849  854  866  879  894  895  926  942  961  990\n",
      "  992  997 1000 1008 1018 1033 1057 1061 1074 1077 1082 1084 1102 1103 1108\n",
      " 1120 1123 1137 1152 1196 1201 1203 1214 1218 1233 1234 1256 1261 1269 1285\n",
      " 1307 1310 1319 1331 1333 1347 1350 1357 1362 1374 1376 1379 1395 1412 1415\n",
      " 1430 1433 1443 1445 1454 1458 1464 1468 1475 1477 1488 1496 1501 1502 1510\n",
      " 1519 1543 1569 1585 1593 1597 1624 1643 1646 1659 1672 1675 1680 1683 1712\n",
      " 1716 1765 1777 1787 1791 1837 1838 1839 1852 1865 1868 1871 1875 1877 1878\n",
      " 1884 1892 1901 1916 1929 1933 1939 1947 1960 1961 1979 1991 2012 2015 2018\n",
      " 2024 2027 2036 2048 2049 2053 2055 2063 2065 2068 2074 2083 2103 2106 2118\n",
      " 2128 2142 2163 2166 2174 2191 2195 2212 2218 2234 2235 2237 2260 2269 2270\n",
      " 2274 2283 2289 2290 2294 2296 2299 2332 2341 2343 2358 2363 2367 2388 2389\n",
      " 2390 2418 2424 2427 2448 2464 2478 2487 2508 2521 2525 2527 2548 2550 2560\n",
      " 2572 2574 2577 2587 2595]\n",
      "TRAIN: [   0    1    3 ..., 2603 2604 2605] TEST: [   2   11   13   17   38   53   79   83   87  119  131  135  144  145  148\n",
      "  172  212  214  215  226  234  243  262  273  274  280  283  303  304  313\n",
      "  324  346  350  357  364  394  407  409  429  437  439  446  451  465  468\n",
      "  473  477  488  495  502  506  514  550  556  597  612  617  618  628  643\n",
      "  653  655  659  660  668  674  694  705  736  754  758  763  765  768  782\n",
      "  783  788  796  802  803  804  806  809  813  818  829  833  839  862  870\n",
      "  872  889  897  909  915  938  994  998 1009 1028 1032 1038 1040 1081 1091\n",
      " 1094 1100 1101 1106 1118 1154 1155 1163 1189 1194 1200 1206 1209 1210 1236\n",
      " 1239 1274 1315 1316 1320 1328 1340 1368 1373 1385 1392 1393 1413 1422 1425\n",
      " 1427 1438 1459 1476 1484 1509 1522 1531 1544 1548 1549 1550 1578 1595 1602\n",
      " 1608 1609 1613 1619 1628 1635 1648 1650 1652 1667 1673 1679 1681 1682 1691\n",
      " 1701 1719 1738 1747 1751 1758 1760 1768 1797 1813 1814 1817 1821 1822 1829\n",
      " 1835 1843 1845 1858 1859 1885 1890 1894 1899 1900 1907 1908 1924 1926 1930\n",
      " 1932 1951 1968 1972 1981 1998 2002 2003 2040 2045 2059 2082 2084 2086 2087\n",
      " 2097 2108 2111 2113 2116 2122 2134 2135 2137 2141 2143 2150 2161 2170 2176\n",
      " 2180 2186 2189 2216 2220 2230 2231 2240 2244 2263 2268 2321 2334 2342 2362\n",
      " 2365 2372 2410 2431 2433 2434 2454 2463 2467 2469 2495 2520 2523 2526 2533\n",
      " 2541 2571 2580 2592 2602]\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2345 TEST: 261\n",
      "TRAIN: 2346 TEST: 260\n",
      "TRAIN: 2346 TEST: 260\n",
      "TRAIN: 2346 TEST: 260\n",
      "TRAIN: 2346 TEST: 260\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "for train_index, test_index in cv.split(data):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to point out here that, because we have TIME SERIES data, the same Census blocks may be appearing in our training AND our test sets. This is a challenge to ensuring that our training and test samples are INDEPENDENT. While Rodent Control does not inspect the same blocks every month, some of the same blocks may be re-inspected from month to month depending on where 311 requests are coming from. \n",
    "\n",
    "However, this also affords us an opportunity. More than likely, when we make predictions about which inspections will lead our inspectors to rat burrows, we are interested in predicting FUTURE inspections with observations from PAST inspections. In this case, cross-validating over time can be a very good way of looking at how well our models are performing. \n",
    "\n",
    "Cross-validating over time requires more than just splitting by month. Rather, we will use observations from each month as a test set and train our models on all PRIOR months. Which we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 61.5\n",
      "Precision: 59.7\n",
      "Precision: 51.8\n",
      "Precision: 55.2\n",
      "Precision: 68.1\n",
      "Precision: 56.1\n",
      "Precision: 64.6\n",
      "Precision: 47.2\n",
      "Precision: 43.8\n",
      "Precision: 50.0\n"
     ]
    }
   ],
   "source": [
    "## Define function\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "\n",
    "## Create for-loop\n",
    "for train_index, test_index in cv.split(data):\n",
    "\n",
    "    ## Define training and test sets\n",
    "    X_train = data.loc[train_index].drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_train = data.loc[train_index]['activity']\n",
    "    X_test = data.loc[test_index].drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_test = data.loc[test_index]['activity']\n",
    "        \n",
    "    ## Fit model\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ## Generate predictions\n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    ## Compare to actual outcomes and return precision\n",
    "    print('Precision: '+str(100 * round(precision_score(y_test, predicted),3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation by Month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by seeing what our cross-validation sets look like. Below, we loop through each of the sets to see which months end up in our training and test sets. You can see that as we move from month to month, we have more and more past observations in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Month: [2] Training Months: [1]\n",
      "Test Month: [3] Training Months: [1 2]\n",
      "Test Month: [4] Training Months: [1 2 3]\n",
      "Test Month: [5] Training Months: [1 2 3 4]\n",
      "Test Month: [6] Training Months: [1 2 3 4 5]\n",
      "Test Month: [7] Training Months: [1 2 3 4 5 6]\n",
      "Test Month: [8] Training Months: [1 2 3 4 5 6 7]\n",
      "Test Month: [9] Training Months: [1 2 3 4 5 6 7 8]\n",
      "Test Month: [10] Training Months: [1 2 3 4 5 6 7 8 9]\n",
      "Test Month: [11] Training Months: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Test Month: [12] Training Months: [ 1  2  3  4  5  6  7  8  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "months = np.sort(data.month.unique())\n",
    "\n",
    "for month in range(2,13):\n",
    "    test = data[data.month==month]\n",
    "    train = data[(data.month < month)]\n",
    "\n",
    "    print('Test Month: '+str(test.month.unique()), 'Training Months: '+str(train.month.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Month 2: 79.2\n",
      "Precision for Month 3: 67.9\n",
      "Precision for Month 4: 48.9\n",
      "Precision for Month 5: 63.5\n",
      "Precision for Month 6: 61.3\n",
      "Precision for Month 7: 67.9\n",
      "Precision for Month 8: 67.2\n",
      "Precision for Month 9: 67.6\n",
      "Precision for Month 10: 57.3\n",
      "Precision for Month 11: 68.3\n",
      "Precision for Month 12: 70.5\n"
     ]
    }
   ],
   "source": [
    "months = np.sort(data.month.unique())\n",
    "\n",
    "for month in range(2,13):\n",
    "\n",
    "    test = data[data.month==month]\n",
    "    train = data[(data.month < month)]\n",
    "    X_test = test.drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_test = test['activity']\n",
    "    X_train = test.drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_train = test['activity']\n",
    "        \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    print('Precision for Month '+str(month)+': '+str(100*round(precision_score(y_test, predicted),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to be performing even better when we cross-validate over months, possibly because we're structuring the cross-validation such that inspections in some of the same blocks appear consistently over time. \n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Try re-creating this cross-validation, but with the training set restricted to only the 3 months prior to the test set. Now do the same with the last 1 and 2 months. Do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Month: [4] Training Months: [1 2 3]\n",
      "Test Month: [5] Training Months: [2 3 4]\n",
      "Test Month: [6] Training Months: [3 4 5]\n",
      "Test Month: [7] Training Months: [4 5 6]\n",
      "Test Month: [8] Training Months: [5 6 7]\n",
      "Test Month: [9] Training Months: [6 7 8]\n",
      "Test Month: [10] Training Months: [7 8 9]\n",
      "Test Month: [11] Training Months: [ 8  9 10]\n",
      "Test Month: [12] Training Months: [ 9 10 11]\n"
     ]
    }
   ],
   "source": [
    "months = np.sort(data.month.unique())\n",
    "\n",
    "for month in range(4,13):\n",
    "    test = data[data.month==month]\n",
    "    train = data[(data.month==month-3)|(data.month==month-2)|(data.month==month-1)]\n",
    "\n",
    "\n",
    "    print('Test Month: '+str(test.month.unique()), 'Training Months: '+str(train.month.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may still be concerned about the independence of our training and test sets. In particular, as I've pointed out, the same Census blocks may appear repeatedly in our data over time. In this case, it may be good to cross-validate geographically to make sure that our model is performing well in different parts of the city. In particular, we know that requests for rodent abatement (and rats themselves) are more common in some parts of the city than in others. In particular, rats are more common in the more densely-populated parts of downtown and less common in less densely-populated places like Wards 3, 7, and 8. Therefore, we may be interested in cross-validating by ward. \n",
    "\n",
    "Again, this is as simple as looping through each of the 8 wards, holding out each ward as a test set and training the models on observations from the remaining wards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate by Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    419\n",
       "2    477\n",
       "3    105\n",
       "4    496\n",
       "5    355\n",
       "6    458\n",
       "7    154\n",
       "8    142\n",
       "Name: WARD, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.WARD.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Ward 1: 66.3\n",
      "Precision for Ward 2: 67.2\n",
      "Precision for Ward 3: 71.8\n",
      "Precision for Ward 4: 57.5\n",
      "Precision for Ward 5: 69.4\n",
      "Precision for Ward 6: 36.0\n",
      "Precision for Ward 7: 25.0\n",
      "Precision for Ward 8: 30.6\n"
     ]
    }
   ],
   "source": [
    "for ward in np.sort(data.WARD.unique()):\n",
    "\n",
    "    test = data[data.WARD == ward]\n",
    "    train = data[data.WARD != ward]\n",
    "    X_test = test.drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_test = test['activity']\n",
    "    X_train = test.drop(['activity', 'month', 'WARD'], axis=1)\n",
    "    y_train = test['activity']\n",
    "        \n",
    "    clf = LogisticRegression(class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    print('Precision for Ward '+str(ward)+': '+str(100*round(precision_score(y_test, predicted),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model performs very well predicting the outcomes of inspections in wards 1 through 4, but less well in wards 5 though 8. In wards 7 and 8 in particular, the model fails to predict any positive cases. This means that our model may be overfit to observations in Wards 1 through 6, and we may want to re-evaluate our approach. \n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Explore the data and our model and try to come up with some reasons that the model is performing poorly on Wards 7 and 8. Is there a way we can fix the model to perform better on those wards? How might we fix the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#because Ward 7 and 8 have more data than other wards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Now try running some cross-validations with the data from your project. What are some different ways you might slice the data you're using for your project? Try them out here. This will be a good way to begin making progress toward your final submission. \n",
    "\n",
    "PLEASE REMEMBER TO SUBMIT THIS HOMEWORK BY CLASS TIME ON THURSDAY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since we are still merging our data, we will do this once we have it."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
